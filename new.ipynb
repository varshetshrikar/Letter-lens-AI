{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TiNJUf4zJIVy",
    "outputId": "bd8b694b-2e14-45db-ae2a-85ce58278eac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
      "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.26.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "poppler-utils is already the newest version (22.02.0-2ubuntu0.5).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
     ]
    }
   ],
   "source": [
    " !pip install pdf2image opencv-python-headless Pillow torch torchvision\n",
    "!apt-get install -y poppler-utils\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow  # For displaying images in Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rwySV3-IqiC"
   },
   "source": [
    "PDF to Image Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MaSQjqXcItTB",
    "outputId": "deefa053-6794-4316-b853-6344cfaa0f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted images: ['/content/images/202211003_page_1.png', '/content/images/202211005_page_1.png', '/content/images/202211007_page_1.png', '/content/images/202211008_page_1.png', '/content/images/202211009_page_1.png']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# Convert PDFs in a folder to images\n",
    "def extract_images_from_pdfs(folder_path, output_folder=\"/content/images\"):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    pdf_files = sorted(glob.glob(os.path.join(folder_path, \"*.pdf\")))\n",
    "\n",
    "    all_image_paths = []\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_name = os.path.splitext(os.path.basename(pdf_file))[0]\n",
    "        images = convert_from_path(pdf_file)\n",
    "\n",
    "        for i, img in enumerate(images):\n",
    "            img_path = os.path.join(output_folder, f\"{pdf_name}_page_{i+1}.png\")\n",
    "            img.save(img_path, \"PNG\")\n",
    "            all_image_paths.append(img_path)\n",
    "\n",
    "    return all_image_paths\n",
    "\n",
    "# Run the function for your PDF folder path\n",
    "pdf_folder_path = \"/content/drive/MyDrive/Colab Notebooks/Design project/Dataset\"\n",
    "image_paths = extract_images_from_pdfs(pdf_folder_path)\n",
    "print(\"Extracted images:\", image_paths[:5])  # Display a few paths for confirmation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bG3LdNJEJSuB"
   },
   "source": [
    "Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efkWVLoUJHc6"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Preprocess images (resize, threshold, etc.)\n",
    "def preprocess_image(img_path):\n",
    "    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (128, 128))  # Resize for consistency\n",
    "    _, binary_image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    return binary_image\n",
    "\n",
    "# Apply preprocessing to the first few images\n",
    "processed_images = [preprocess_image(path) for path in image_paths[:5]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wt3RPPOJlNF"
   },
   "source": [
    "Feature Encoding: Convert Handwriting Features to Binary-Coded Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gi5ySeBJl_n"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example of feature extraction function\n",
    "def extract_features_from_image(binary_image):\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    features = []\n",
    "\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        baseline = \"ascending\" if y + h < binary_image.shape[0] / 2 else \"descending\" if y > binary_image.shape[0] / 2 else \"leveled\"\n",
    "        connectivity = \"strongly connected\" if w / h > 1.5 else \"medium connectivity\" if w / h > 0.5 else \"not connected\"\n",
    "        angle = np.arctan2(h, w) * 180 / np.pi\n",
    "        slant = \"vertical\" if angle < 5 else \"moderate right\" if angle < 15 else \"extreme right\" if angle < 45 else \"moderate left\" if angle > 175 else \"extreme left\"\n",
    "        area = cv2.contourArea(contour)\n",
    "        pressure = \"heavy\" if area > 1000 else \"medium\" if area > 300 else \"light\"\n",
    "        t_height = \"not t\"  # Logic for detecting 't'\n",
    "        f_shape = \"not f\"  # Logic for detecting 'f'\n",
    "        spacing = \"evenly spaced\"  # Logic to assess spacing\n",
    "\n",
    "        features.append(generate_hm_row(baseline, connectivity, slant, pressure, t_height, f_shape, spacing))\n",
    "\n",
    "    return features\n",
    "\n",
    "def generate_hm_row(baseline, connectivity, slant, pressure, t_height, f_shape, spacing):\n",
    "    baseline_code = 0 if baseline == \"ascending\" else 1 if baseline == \"descending\" else 2\n",
    "    connectivity_code = 0 if connectivity == \"strongly connected\" else 1 if connectivity == \"medium connectivity\" else 2\n",
    "    slant_code = 0 if slant == \"vertical\" else 1 if slant == \"moderate right\" else 2 if slant == \"extreme right\" else 3 if slant == \"moderate left\" else 4\n",
    "    pressure_code = 0 if pressure == \"light\" else 1 if pressure == \"medium\" else 2\n",
    "    t_height_code = 0 if t_height == \"not t\" else 1 if t_height == \"very low\" else 2 if t_height == \"very high\" else 3\n",
    "    f_shape_code = 0 if f_shape == \"not f\" else 1\n",
    "    spacing_code = 0 if spacing == \"evenly spaced\" else 1\n",
    "\n",
    "    return [baseline_code, connectivity_code, slant_code, pressure_code, t_height_code, f_shape_code, spacing_code]\n",
    "\n",
    "# Process images to extract features and create the handwriting map (HM)\n",
    "handwriting_map = [extract_features_from_image(preprocess_image(path)) for path in image_paths]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcTOWSZBJtQd"
   },
   "source": [
    "Trait Extraction: Calculate and Normalize Big Five Trait Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IDgeWk91JuTf",
    "outputId": "2ef63ae5-afe9-4908-c044-2fb4e37440a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extraversion  Agreeableness  Conscientiousness  Neuroticism  Openness\n",
      "0      0.600000       0.600000           0.600000     0.600000  0.600000\n",
      "1      0.666667       0.600000           0.600000     0.800000  0.800000\n",
      "2      0.466667       0.866667           0.866667     0.733333  1.000000\n",
      "3      0.400000       0.466667           0.400000     1.000000  0.800000\n",
      "4      0.333333       0.733333           1.000000     0.333333  0.933333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV with Big Five responses\n",
    "file_path = '/content/drive/MyDrive/Colab Notebooks/Design project/Personality traits/Updated response.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Define columns for each personality trait\n",
    "trait_columns = {\n",
    "    \"Extraversion\": [\"I see myself as someone who is talkative.\", \"I see myself as someone who is full of energy.\", \"I see myself as someone who is outgoing, sociable.\"],\n",
    "    \"Agreeableness\": [\"I see myself as someone who is helpful and unselfish with others.\", \"I see myself as someone who is considerate and kind to almost everyone.\", \"I see myself as someone who likes to cooperate with others.\"],\n",
    "    \"Conscientiousness\": [\"I see myself as someone who does a thorough job.\", \"I see myself as someone who makes plans and follows through with them.\", \"I see myself as someone who does things efficiently.\"],\n",
    "    \"Neuroticism\": [\"I see myself as someone who worries a lot.\", \"I see myself as someone who gets nervous easily.\", \"I see myself as someone who can be moody.\"],\n",
    "    \"Openness\": [\"I see myself as someone who is original, comes up with new ideas.\", \"I see myself as someone who likes to reflect, play with ideas.\", \"I see myself as someone who is curious about many different things.\"]\n",
    "}\n",
    "\n",
    "# Calculate average scores and normalize\n",
    "trait_averages = pd.DataFrame({trait: data[cols].mean(axis=1) for trait, cols in trait_columns.items()})\n",
    "trait_averages_normalized = trait_averages / 5\n",
    "print(trait_averages_normalized.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCDEspWDJ1lF"
   },
   "source": [
    "Dataset Preparation: Combine Features and Traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upOrISi8J2a6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HandwritingPersonalityDataset(Dataset):\n",
    "    def __init__(self, handwriting_map, trait_scores, max_letters=70):\n",
    "        self.handwriting_map = handwriting_map\n",
    "        self.trait_scores = trait_scores.values  # Convert DataFrame to numpy array\n",
    "        self.max_letters = max_letters\n",
    "        self.feature_size = 7  # Each letter has 7 binary-coded features\n",
    "        self.total_features = 1610\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.handwriting_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get handwriting features and ensure consistent shape\n",
    "        handwriting_features = np.array(self.handwriting_map[idx])\n",
    "\n",
    "        # Pad or truncate the handwriting features to max_letters (70)\n",
    "        if handwriting_features.shape[0] < self.max_letters:\n",
    "            padding = np.zeros((self.max_letters - handwriting_features.shape[0], self.feature_size))\n",
    "            handwriting_features = np.vstack([handwriting_features, padding])\n",
    "        elif handwriting_features.shape[0] > self.max_letters:\n",
    "            handwriting_features = handwriting_features[:self.max_letters]\n",
    "\n",
    "        # Flatten to a single vector of size 490\n",
    "        handwriting_features = handwriting_features.flatten()\n",
    "\n",
    "        # Pad to reach exactly 1610 elements\n",
    "        if handwriting_features.size < self.total_features:\n",
    "            handwriting_features = np.pad(handwriting_features, (0, self.total_features - handwriting_features.size), mode='constant')\n",
    "\n",
    "        # Convert to tensor\n",
    "        handwriting_features = torch.tensor(handwriting_features, dtype=torch.float32)\n",
    "        traits = torch.tensor(self.trait_scores[idx], dtype=torch.float32)\n",
    "\n",
    "        return handwriting_features, traits\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = HandwritingPersonalityDataset(handwriting_map, trait_averages_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObV2MfGSKBwG"
   },
   "source": [
    "5. Model Building and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32xfyzhnKCg3"
   },
   "source": [
    "FFM-NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U8mMAuPKFCN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FFM_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFM_NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1610, 1850)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(1850, 5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = FFM_NN()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKOp-cQeKJ5-"
   },
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wa4ROLsjKKZ0",
    "outputId": "03e20d03-0c22-4059-ba2a-f1c9345e683f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([1, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.06542123109102249\n",
      "Epoch 2/10, Loss: 0.07645122706890106\n",
      "Epoch 3/10, Loss: 0.07874710857868195\n",
      "Epoch 4/10, Loss: 0.07948936522006989\n",
      "Epoch 5/10, Loss: 0.07990677654743195\n",
      "Epoch 6/10, Loss: 0.0802365317940712\n",
      "Epoch 7/10, Loss: 0.08052666485309601\n",
      "Epoch 8/10, Loss: 0.0807863399386406\n",
      "Epoch 9/10, Loss: 0.0810171514749527\n",
      "Epoch 10/10, Loss: 0.0812198668718338\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.4)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for features, labels in dataset:\n",
    "        # Ensure feature vector has correct shape\n",
    "        features = features.view(1, -1)  # Reshape to (1, 1610) if needed\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAEqGhb0MlT6"
   },
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiusOYJPMkGU"
   },
   "outputs": [],
   "source": [
    "# After training\n",
    "torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/Design project/personality_traits_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apxhH9uaMsQ_",
    "outputId": "0898c6a1-fc97-44f4-9af1-ed4e0cbd238d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-d09351c0a5e3>:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Personality Traits:\n",
      "Extraversion: 0.56\n",
      "Agreeableness: 0.61\n",
      "Conscientiousness: 0.58\n",
      "Neuroticism: 0.57\n",
      "Openness: 0.60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Extraversion': 0.5600668,\n",
       " 'Agreeableness': 0.6127986,\n",
       " 'Conscientiousness': 0.5787232,\n",
       " 'Neuroticism': 0.5724024,\n",
       " 'Openness': 0.6006152}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# Function to convert a PDF to images\n",
    "def extract_images_from_pdf(pdf_path):\n",
    "    images = convert_from_path(pdf_path)\n",
    "    return images\n",
    "\n",
    "# Function to preprocess an image for model input\n",
    "def preprocess_image(img):\n",
    "    # Resize and convert to grayscale\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    return binary_image\n",
    "\n",
    "# Function to extract features from a binary image and generate HM row\n",
    "def extract_features_and_generate_hm_row(binary_image):\n",
    "    # Implement feature extraction similar to training\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Placeholder for detected features\n",
    "    baseline, connecting_strokes, slant, pressure, t_height, f_shape, spacing = (\n",
    "        \"leveled\", \"medium connectivity\", \"vertical\", \"medium\", \"not t\", \"not f\", \"evenly spaced\"\n",
    "    )\n",
    "\n",
    "    # Here you'd normally extract actual feature values from the image\n",
    "    # But for simplicity, let's assume these default values for this example\n",
    "\n",
    "    # Generate HM row using the same encoding logic as during training\n",
    "    hm_row = generate_hm_row(baseline, connecting_strokes, slant, pressure, t_height, f_shape, spacing)\n",
    "    return hm_row\n",
    "\n",
    "# Function to make predictions on a new PDF\n",
    "def predict_personality_traits(model, pdf_path):\n",
    "    # Convert PDF to images\n",
    "    images = extract_images_from_pdf(pdf_path)\n",
    "    hm_rows = []\n",
    "\n",
    "    # Process each image page\n",
    "    for img in images:\n",
    "        binary_image = preprocess_image(np.array(img))\n",
    "        hm_row = extract_features_and_generate_hm_row(binary_image)\n",
    "        hm_rows.append(hm_row)\n",
    "\n",
    "    # Stack all HM rows and pad to required input shape if needed\n",
    "    # Ensure hm_rows is 2D and convert to tensor\n",
    "    input_data = torch.tensor(hm_rows, dtype=torch.float32).view(1, -1)  # Reshape to match model input\n",
    "\n",
    "    # Make sure input size is correct\n",
    "    if input_data.shape[1] < 1610:\n",
    "        padding = torch.zeros((1, 1610 - input_data.shape[1]))  # Zero-pad to match model input size\n",
    "        input_data = torch.cat((input_data, padding), dim=1)\n",
    "    elif input_data.shape[1] > 1610:\n",
    "        input_data = input_data[:, :1610]  # Trim if it's longer\n",
    "\n",
    "    # Model prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_data)\n",
    "\n",
    "    # Convert prediction to numpy and print results\n",
    "    traits = [\"Extraversion\", \"Agreeableness\", \"Conscientiousness\", \"Neuroticism\", \"Openness\"]\n",
    "    trait_scores = prediction.numpy().flatten()\n",
    "    result = dict(zip(traits, trait_scores))\n",
    "\n",
    "    print(\"Predicted Personality Traits:\")\n",
    "    for trait, score in result.items():\n",
    "        print(f\"{trait}: {score:.2f}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# Path to your trained model file\n",
    "model_path = '/content/drive/MyDrive/Colab Notebooks/Design project/personality_traits_model.pth'\n",
    "model = FFM_NN()  # Initialize your model class\n",
    "model.load_state_dict(torch.load(model_path))  # Load model weights\n",
    "\n",
    "# Predict personality traits from a PDF\n",
    "pdf_path = '/content/drive/MyDrive/Colab Notebooks/Design project/Dataset/202211067.pdf'  # Path to the new PDF file\n",
    "predict_personality_traits(model, pdf_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgsVKJHuOACf"
   },
   "source": [
    "Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKO-GXsuOCWY",
    "outputId": "c8644c37-9ba6-41d4-f834-1ef7b4106797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraversion - MSE: 0.0265, MAE: 0.1319, R2: 0.0376\n",
      "Agreeableness - MSE: 0.0231, MAE: 0.1180, R2: 0.0322\n",
      "Conscientiousness - MSE: 0.0215, MAE: 0.1139, R2: 0.0097\n",
      "Neuroticism - MSE: 0.0387, MAE: 0.1606, R2: -0.0056\n",
      "Openness - MSE: 0.0197, MAE: 0.1083, R2: 0.0213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "\n",
    "# Assuming `dataset` is a DataLoader with test data, similar to the training set\n",
    "model.eval()  # Set model to evaluation mode\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in dataset:\n",
    "        predictions = model(features).squeeze(0)  # Generate predictions\n",
    "        all_labels.append(labels.numpy())         # Collect actual labels\n",
    "        all_predictions.append(predictions.numpy()) # Collect predictions\n",
    "\n",
    "# Convert lists to numpy arrays for metrics calculation\n",
    "all_labels = np.vstack(all_labels)\n",
    "all_predictions = np.vstack(all_predictions)\n",
    "\n",
    "# Calculate MSE, MAE, and R-squared for each trait\n",
    "mse = mean_squared_error(all_labels, all_predictions, multioutput='raw_values')\n",
    "mae = mean_absolute_error(all_labels, all_predictions, multioutput='raw_values')\n",
    "r2 = r2_score(all_labels, all_predictions, multioutput='raw_values')\n",
    "\n",
    "# Print metrics for each trait\n",
    "traits = [\"Extraversion\", \"Agreeableness\", \"Conscientiousness\", \"Neuroticism\", \"Openness\"]\n",
    "for i, trait in enumerate(traits):\n",
    "    print(f\"{trait} - MSE: {mse[i]:.4f}, MAE: {mae[i]:.4f}, R2: {r2[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-bqqCAgwPkCE",
    "outputId": "0f3b15df-198c-4fa5-d043-99ffa716ec98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each trait:\n",
      "Extraversion: 1\n",
      "Agreeableness: 1\n",
      "Conscientiousness: 1\n",
      "Neuroticism: 1\n",
      "Openness: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-e8315cd28c2a>:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Design project/personality_traits_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/content/drive/MyDrive/Colab Notebooks/Design project/Personality traits/Updated response.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Strip whitespace from column names\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Define columns for each personality trait\n",
    "trait_columns = {\n",
    "    \"Extraversion\": [\n",
    "        \"I see myself as someone who is talkative.\",\n",
    "        \"I see myself as someone who is full of energy.\",\n",
    "        \"I see myself as someone who is outgoing, sociable.\"\n",
    "    ],\n",
    "    \"Agreeableness\": [\n",
    "        \"I see myself as someone who is helpful and unselfish with others.\",\n",
    "        \"I see myself as someone who is considerate and kind to almost everyone.\",\n",
    "        \"I see myself as someone who likes to cooperate with others.\"\n",
    "    ],\n",
    "    \"Conscientiousness\": [\n",
    "        \"I see myself as someone who does a thorough job.\",\n",
    "        \"I see myself as someone who makes plans and follows through with them.\",\n",
    "        \"I see myself as someone who does things efficiently.\"\n",
    "    ],\n",
    "    \"Neuroticism\": [\n",
    "        \"I see myself as someone who worries a lot.\",\n",
    "        \"I see myself as someone who gets nervous easily.\",\n",
    "        \"I see myself as someone who can be moody.\"\n",
    "    ],\n",
    "    \"Openness\": [\n",
    "        \"I see myself as someone who is original, comes up with new ideas.\",\n",
    "        \"I see myself as someone who likes to reflect, play with ideas.\",\n",
    "        \"I see myself as someone who is curious about many different things.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Calculate the average score for each trait\n",
    "trait_averages = pd.DataFrame()\n",
    "for trait, cols in trait_columns.items():\n",
    "    trait_averages[trait] = data[cols].mean(axis=1)\n",
    "\n",
    "# Normalize the averages to be between 0 and 1\n",
    "trait_averages_normalized = trait_averages / 5\n",
    "\n",
    "# Define the model class\n",
    "class FFM_NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FFM_NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 1610  # Update this with the actual number of input features\n",
    "hidden_size = 1850\n",
    "output_size = 5\n",
    "model = FFM_NN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Load the trained model (assuming you saved it as 'personality_model.pth')\n",
    "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Design project/personality_traits_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Function to predict personality traits based on handwriting features\n",
    "def predict_personality(features):\n",
    "    features = torch.tensor(features, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(features)\n",
    "    return outputs.numpy()\n",
    "\n",
    "# Example: Predict for some features (replace with actual handwriting features)\n",
    "# Assuming you have a list of features for each sample (e.g., 1610 features for each sample)\n",
    "sample_features = np.random.rand(1, 1610)  # Replace with actual feature data from your dataset\n",
    "predictions = predict_personality(sample_features)\n",
    "\n",
    "# Convert predictions to categories\n",
    "def convert_to_category(score, thresholds):\n",
    "    if score < thresholds[1]:\n",
    "        return 0  # Low\n",
    "    elif score < thresholds[2]:\n",
    "        return 1  # Medium\n",
    "    else:\n",
    "        return 2  # High\n",
    "\n",
    "# Define thresholds for each trait: Low, Medium, High\n",
    "thresholds = {\n",
    "    'Extraversion': [0.0, 0.33, 0.66, 1.0],  # Low, Medium, High\n",
    "    'Agreeableness': [0.0, 0.33, 0.66, 1.0],\n",
    "    'Conscientiousness': [0.0, 0.33, 0.66, 1.0],\n",
    "    'Neuroticism': [0.0, 0.33, 0.66, 1.0],\n",
    "    'Openness': [0.0, 0.33, 0.66, 1.0]\n",
    "}\n",
    "\n",
    "# Convert predicted traits to categories (treat predictions as scalars)\n",
    "predicted_traits = {\n",
    "    'Extraversion': predictions[0][0],  # Assuming predictions are scalar\n",
    "    'Agreeableness': predictions[0][1],\n",
    "    'Conscientiousness': predictions[0][2],\n",
    "    'Neuroticism': predictions[0][3],\n",
    "    'Openness': predictions[0][4]\n",
    "}\n",
    "\n",
    "# Convert the predictions and actual values into categories\n",
    "predicted_categories = {trait: convert_to_category(pred, thresholds[trait]) for trait, pred in predicted_traits.items()}\n",
    "actual_categories = {trait: convert_to_category(value, thresholds[trait]) for trait, value in trait_averages_normalized.iloc[0].items()}\n",
    "\n",
    "# Calculate accuracy for each trait\n",
    "accuracy = {trait: int(predicted_categories[trait] == actual_categories[trait]) for trait in predicted_categories}\n",
    "\n",
    "print(\"Accuracy for each trait:\")\n",
    "for trait, acc in accuracy.items():\n",
    "    print(f\"{trait}: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "# import torchvision.transforms as transforms\n",
    "# from pdf2image import convert_from_path\n",
    "# import os\n",
    "# import shutil\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the original CSV with trait averages\n",
    "# file_path = '/content/drive/MyDrive/Design Project/Updated response.csv'\n",
    "# data = pd.read_csv(file_path)\n",
    "# data.columns = data.columns.str.strip()  # Remove any extra whitespace\n",
    "\n",
    "# # Define the trait columns for extraction\n",
    "# trait_columns = {\n",
    "#     \"Extraversion\": [\"I see myself as someone who is talkative.\", \"I see myself as someone who is full of energy.\", \"I see myself as someone who is outgoing, sociable.\"],\n",
    "#     \"Agreeableness\": [\"I see myself as someone who is helpful and unselfish with others.\", \"I see myself as someone who is considerate and kind to almost everyone.\", \"I see myself as someone who likes to cooperate with others.\"],\n",
    "#     \"Conscientiousness\": [\"I see myself as someone who does a thorough job.\", \"I see myself as someone who makes plans and follows through with them.\", \"I see myself as someone who does things efficiently.\"],\n",
    "#     \"Neuroticism\": [\"I see myself as someone who worries a lot.\", \"I see myself as someone who gets nervous easily.\", \"I see myself as someone who can be moody.\"],\n",
    "#     \"Openness\": [\"I see myself as someone who is original, comes up with new ideas.\", \"I see myself as someone who likes to reflect, play with ideas.\", \"I see myself as someone who is curious about many different things.\"]\n",
    "# }\n",
    "\n",
    "# # Calculate the average score for each trait and normalize\n",
    "# trait_averages = pd.DataFrame()\n",
    "# for trait, cols in trait_columns.items():\n",
    "#     trait_averages[trait] = data[cols].mean(axis=1)\n",
    "\n",
    "# # Normalize the averages to be between 0 and 1\n",
    "# trait_averages_normalized = trait_averages / 5\n",
    "\n",
    "# # Get the list of labels for each person (ground truth)\n",
    "# trait_labels = trait_averages_normalized.values.tolist()\n",
    "\n",
    "# # Ensure each page in the PDF corresponds to a person in the CSV (assuming the pages are in the same order)\n",
    "# # Extract the correct trait labels for testing purposes (assuming a 1:1 mapping between pages and individuals in the CSV)\n",
    "# # The assumption here is that each PDF corresponds to one individual's traits (this may need to be adjusted based on your dataset).\n",
    "\n",
    "# # Define the model class again if not already defined\n",
    "# class HandwritingPersonalityModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(HandwritingPersonalityModel, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # 3 channels for RGB images\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.fc1 = nn.Linear(128 * 28 * 28, 512)  # Adjust size based on input image dimensions\n",
    "#         self.fc2 = nn.Linear(512, 5)  # 5 output neurons for Big Five Traits\n",
    "\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.maxpool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.conv1(x))\n",
    "#         x = self.maxpool(x)\n",
    "#         x = self.relu(self.conv2(x))\n",
    "#         x = self.maxpool(x)\n",
    "#         x = self.relu(self.conv3(x))\n",
    "#         x = self.maxpool(x)\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)  # Output 5 trait predictions\n",
    "#         return x\n",
    "\n",
    "# # Load the trained model\n",
    "# model_path = \"/content/drive/MyDrive/Design Project/model_state.pth\"  # Path where the model is saved\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = HandwritingPersonalityModel()\n",
    "# model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "# model.to(device)\n",
    "# model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# # Define the preprocessing function\n",
    "# def preprocess_image(img_path):\n",
    "#     preprocess = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "#         transforms.ToTensor(),         # Convert image to tensor\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "#     ])\n",
    "#     image = Image.open(img_path).convert(\"RGB\")  # Ensure image is in RGB mode\n",
    "#     return preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "\n",
    "# # Function to extract images from PDF\n",
    "# def extract_images_from_pdf(pdf_path, output_folder=\"temp_images\"):\n",
    "#     if os.path.exists(output_folder):\n",
    "#         shutil.rmtree(output_folder)\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#     images = convert_from_path(pdf_path)\n",
    "#     image_paths = []\n",
    "#     for i, image in enumerate(images):\n",
    "#         img_path = os.path.join(output_folder, f\"page_{i+1}.png\")\n",
    "#         image.save(img_path, \"PNG\")\n",
    "#         image_paths.append(img_path)\n",
    "#     return image_paths\n",
    "\n",
    "\n",
    "# # Function to generate predictions and compare them with the true labels (trait_averages_normalized)\n",
    "# def predict_traits_from_pdf(pdf_path, ground_truth_labels):\n",
    "#     # Extract images from the PDF\n",
    "#     image_paths = extract_images_from_pdf(pdf_path)\n",
    "#     predictions_list = []\n",
    "#     ground_truth_dict = {}\n",
    "\n",
    "#     for i, img_path in enumerate(image_paths):\n",
    "#         # Preprocess the image\n",
    "#         image_tensor = preprocess_image(img_path).to(device)\n",
    "\n",
    "#         # Make predictions\n",
    "#         with torch.no_grad():\n",
    "#             output = model(image_tensor)\n",
    "\n",
    "#         # Convert the output to CPU and extract the numpy array\n",
    "#         predictions = output.cpu().numpy()[0]\n",
    "\n",
    "#         # Map predictions to Big Five traits\n",
    "#         traits = [\"Extraversion\", \"Agreeableness\", \"Conscientiousness\", \"Neuroticism\", \"Openness\"]\n",
    "#         predicted_traits = dict(zip(traits, predictions))\n",
    "#         predictions_list.append(predicted_traits)\n",
    "\n",
    "#         # Compare with the corresponding ground truth (labels)\n",
    "#         if i < len(ground_truth_labels):  # Ensure we don't go out of bounds\n",
    "#             ground_truth_dict[f\"Page {i+1}\"] = {\n",
    "#                 \"Predicted\": predicted_traits,\n",
    "#                 \"Ground Truth\": ground_truth_labels[i]\n",
    "#             }\n",
    "\n",
    "#     return ground_truth_dict\n",
    "\n",
    "# # Example usage\n",
    "# pdf_path = \"/content/drive/MyDrive/Design Project/Dataset/202211008.pdf\"  # Path to a new PDF\n",
    "# ground_truth_comparison = predict_traits_from_pdf(pdf_path, trait_labels)\n",
    "\n",
    "# # Print the comparison of predicted and actual traits for each page\n",
    "# for page, comparison in ground_truth_comparison.items():\n",
    "#     print(f\"{page}:\")\n",
    "#     print(f\"  Predicted Traits: {comparison['Predicted']}\")\n",
    "#     print(f\"  Ground Truth: {comparison['Ground Truth']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\graphviz\\backend\\execute.py:78\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m         proc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun(cmd, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py:503\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    501\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py:1456\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1456\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1457\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1458\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1466\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m dot\u001b[38;5;241m.\u001b[39medge(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput Prediction\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Render and display the diagram\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mdot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpersonality_prediction_diagram\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Display the diagram (if running in Jupyter notebook)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\graphviz\\rendering.py:122\u001b[0m, in \u001b[0;36mRender.render\u001b[1;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[0;32m    118\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(filename, directory\u001b[38;5;241m=\u001b[39mdirectory, skip_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    120\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(filepath)\n\u001b[1;32m--> 122\u001b[0m rendered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[0;32m    125\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelete \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, filepath)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\graphviz\\backend\\rendering.py:326\u001b[0m, in \u001b[0;36mrender\u001b[1;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[0;32m    322\u001b[0m cmd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m filepath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork around pytype false alarm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 326\u001b[0m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mfspath(outfile)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\graphviz\\backend\\execute.py:81\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstderr:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "\n",
    "import graphviz\n",
    "\n",
    "# Create a Graphviz object\n",
    "dot = graphviz.Digraph(format='png', engine='dot')\n",
    "\n",
    "# Add nodes for the various parts of the model\n",
    "dot.node('A', 'Handwriting Data\\n(Handwritten Images)')\n",
    "dot.node('B', 'Personality Data\\n(Big Five Trait Scores)')\n",
    "dot.node('C', 'Handwriting Feature\\nExtraction (ResNet-50)')\n",
    "dot.node('D', 'Personality Trait\\nProcessing (Fully Connected Network)')\n",
    "dot.node('E', 'Concatenate Handwriting\\nFeatures + Personality Traits')\n",
    "dot.node('F', 'Final Fully Connected\\nLayer (Prediction Layer)')\n",
    "dot.node('G', 'Output: 5 Big Five\\nPersonality Traits')\n",
    "\n",
    "# Add edges between the nodes to represent the flow\n",
    "dot.edge('A', 'C', label='Extract Features (CNN)')\n",
    "dot.edge('B', 'D', label='Process Traits (FCN)')\n",
    "dot.edge('C', 'E', label='Concatenate Features')\n",
    "dot.edge('D', 'E', label='Concatenate Features')\n",
    "dot.edge('E', 'F', label='Feed into Fully Connected Layer')\n",
    "dot.edge('F', 'G', label='Output Prediction')\n",
    "\n",
    "# Render and display the diagram\n",
    "dot.render('personality_prediction_diagram')\n",
    "\n",
    "# Display the diagram (if running in Jupyter notebook)\n",
    "from IPython.display import Image\n",
    "Image(filename='personality_prediction_diagram.png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
