# -*- coding: utf-8 -*-
"""new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fD5SUOitaFMwZx2DaemY6gIE8-B5TZ3k
"""

!pip install pdf2image opencv-python-headless Pillow torch torchvision
!apt-get install -y poppler-utils
import cv2
from google.colab.patches import cv2_imshow  # For displaying images in Colab

"""PDF to Image Conversion"""

import os
import glob
from pdf2image import convert_from_path

# Convert PDFs in a folder to images
def extract_images_from_pdfs(folder_path, output_folder="/content/images"):
    os.makedirs(output_folder, exist_ok=True)
    pdf_files = sorted(glob.glob(os.path.join(folder_path, "*.pdf")))

    all_image_paths = []
    for pdf_file in pdf_files:
        pdf_name = os.path.splitext(os.path.basename(pdf_file))[0]
        images = convert_from_path(pdf_file)

        for i, img in enumerate(images):
            img_path = os.path.join(output_folder, f"{pdf_name}_page_{i+1}.png")
            img.save(img_path, "PNG")
            all_image_paths.append(img_path)

    return all_image_paths

# Run the function for your PDF folder path
pdf_folder_path = "/content/drive/MyDrive/Colab Notebooks/Design project/Dataset"
image_paths = extract_images_from_pdfs(pdf_folder_path)
print("Extracted images:", image_paths[:5])  # Display a few paths for confirmation

"""Image Preprocessing"""

import cv2

# Preprocess images (resize, threshold, etc.)
def preprocess_image(img_path):
    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    image = cv2.resize(image, (128, 128))  # Resize for consistency
    _, binary_image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    return binary_image

# Apply preprocessing to the first few images
processed_images = [preprocess_image(path) for path in image_paths[:5]]

"""Feature Encoding: Convert Handwriting Features to Binary-Coded Rows"""

import numpy as np

# Example of feature extraction function
def extract_features_from_image(binary_image):
    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    features = []

    for contour in contours:
        x, y, w, h = cv2.boundingRect(contour)
        baseline = "ascending" if y + h < binary_image.shape[0] / 2 else "descending" if y > binary_image.shape[0] / 2 else "leveled"
        connectivity = "strongly connected" if w / h > 1.5 else "medium connectivity" if w / h > 0.5 else "not connected"
        angle = np.arctan2(h, w) * 180 / np.pi
        slant = "vertical" if angle < 5 else "moderate right" if angle < 15 else "extreme right" if angle < 45 else "moderate left" if angle > 175 else "extreme left"
        area = cv2.contourArea(contour)
        pressure = "heavy" if area > 1000 else "medium" if area > 300 else "light"
        t_height = "not t"  # Logic for detecting 't'
        f_shape = "not f"  # Logic for detecting 'f'
        spacing = "evenly spaced"  # Logic to assess spacing

        features.append(generate_hm_row(baseline, connectivity, slant, pressure, t_height, f_shape, spacing))

    return features

def generate_hm_row(baseline, connectivity, slant, pressure, t_height, f_shape, spacing):
    baseline_code = 0 if baseline == "ascending" else 1 if baseline == "descending" else 2
    connectivity_code = 0 if connectivity == "strongly connected" else 1 if connectivity == "medium connectivity" else 2
    slant_code = 0 if slant == "vertical" else 1 if slant == "moderate right" else 2 if slant == "extreme right" else 3 if slant == "moderate left" else 4
    pressure_code = 0 if pressure == "light" else 1 if pressure == "medium" else 2
    t_height_code = 0 if t_height == "not t" else 1 if t_height == "very low" else 2 if t_height == "very high" else 3
    f_shape_code = 0 if f_shape == "not f" else 1
    spacing_code = 0 if spacing == "evenly spaced" else 1

    return [baseline_code, connectivity_code, slant_code, pressure_code, t_height_code, f_shape_code, spacing_code]

# Process images to extract features and create the handwriting map (HM)
handwriting_map = [extract_features_from_image(preprocess_image(path)) for path in image_paths]

"""Trait Extraction: Calculate and Normalize Big Five Trait Scores"""

import pandas as pd

# Load CSV with Big Five responses
file_path = '/content/drive/MyDrive/Colab Notebooks/Design project/Personality traits/Updated response.csv'
data = pd.read_csv(file_path)
data.columns = data.columns.str.strip()

# Define columns for each personality trait
trait_columns = {
    "Extraversion": ["I see myself as someone who is talkative.", "I see myself as someone who is full of energy.", "I see myself as someone who is outgoing, sociable."],
    "Agreeableness": ["I see myself as someone who is helpful and unselfish with others.", "I see myself as someone who is considerate and kind to almost everyone.", "I see myself as someone who likes to cooperate with others."],
    "Conscientiousness": ["I see myself as someone who does a thorough job.", "I see myself as someone who makes plans and follows through with them.", "I see myself as someone who does things efficiently."],
    "Neuroticism": ["I see myself as someone who worries a lot.", "I see myself as someone who gets nervous easily.", "I see myself as someone who can be moody."],
    "Openness": ["I see myself as someone who is original, comes up with new ideas.", "I see myself as someone who likes to reflect, play with ideas.", "I see myself as someone who is curious about many different things."]
}

# Calculate average scores and normalize
trait_averages = pd.DataFrame({trait: data[cols].mean(axis=1) for trait, cols in trait_columns.items()})
trait_averages_normalized = trait_averages / 5
print(trait_averages_normalized.head())

"""Dataset Preparation: Combine Features and Traits"""

import numpy as np
import torch
from torch.utils.data import Dataset

class HandwritingPersonalityDataset(Dataset):
    def __init__(self, handwriting_map, trait_scores, max_letters=70):
        self.handwriting_map = handwriting_map
        self.trait_scores = trait_scores.values  # Convert DataFrame to numpy array
        self.max_letters = max_letters
        self.feature_size = 7  # Each letter has 7 binary-coded features
        self.total_features = 1610

    def __len__(self):
        return len(self.handwriting_map)

    def __getitem__(self, idx):
        # Get handwriting features and ensure consistent shape
        handwriting_features = np.array(self.handwriting_map[idx])

        # Pad or truncate the handwriting features to max_letters (70)
        if handwriting_features.shape[0] < self.max_letters:
            padding = np.zeros((self.max_letters - handwriting_features.shape[0], self.feature_size))
            handwriting_features = np.vstack([handwriting_features, padding])
        elif handwriting_features.shape[0] > self.max_letters:
            handwriting_features = handwriting_features[:self.max_letters]

        # Flatten to a single vector of size 490
        handwriting_features = handwriting_features.flatten()

        # Pad to reach exactly 1610 elements
        if handwriting_features.size < self.total_features:
            handwriting_features = np.pad(handwriting_features, (0, self.total_features - handwriting_features.size), mode='constant')

        # Convert to tensor
        handwriting_features = torch.tensor(handwriting_features, dtype=torch.float32)
        traits = torch.tensor(self.trait_scores[idx], dtype=torch.float32)

        return handwriting_features, traits

# Create dataset instance
dataset = HandwritingPersonalityDataset(handwriting_map, trait_averages_normalized)

"""5. Model Building and Training

FFM-NN Architecture
"""

import torch
import torch.nn as nn

class FFM_NN(nn.Module):
    def __init__(self):
        super(FFM_NN, self).__init__()
        self.fc1 = nn.Linear(1610, 1850)
        self.tanh = nn.Tanh()
        self.fc2 = nn.Linear(1850, 5)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.tanh(self.fc1(x))
        x = self.sigmoid(self.fc2(x))
        return x

# Instantiate the model
model = FFM_NN()

"""Model Training"""

import torch.optim as optim

# Define the loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.4)

# Training loop
epochs = 10
for epoch in range(epochs):
    for features, labels in dataset:
        # Ensure feature vector has correct shape
        features = features.view(1, -1)  # Reshape to (1, 1610) if needed

        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}")

"""Save the model"""

# After training
torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/Design project/personality_traits_model.pth')

import torch
import cv2
import numpy as np
from pdf2image import convert_from_path

# Function to convert a PDF to images
def extract_images_from_pdf(pdf_path):
    images = convert_from_path(pdf_path)
    return images

# Function to preprocess an image for model input
def preprocess_image(img):
    # Resize and convert to grayscale
    img = cv2.resize(img, (128, 128))
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    _, binary_image = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    return binary_image

# Function to extract features from a binary image and generate HM row
def extract_features_and_generate_hm_row(binary_image):
    # Implement feature extraction similar to training
    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    # Placeholder for detected features
    baseline, connecting_strokes, slant, pressure, t_height, f_shape, spacing = (
        "leveled", "medium connectivity", "vertical", "medium", "not t", "not f", "evenly spaced"
    )

    # Here you'd normally extract actual feature values from the image
    # But for simplicity, let's assume these default values for this example

    # Generate HM row using the same encoding logic as during training
    hm_row = generate_hm_row(baseline, connecting_strokes, slant, pressure, t_height, f_shape, spacing)
    return hm_row

# Function to make predictions on a new PDF
def predict_personality_traits(model, pdf_path):
    # Convert PDF to images
    images = extract_images_from_pdf(pdf_path)
    hm_rows = []

    # Process each image page
    for img in images:
        binary_image = preprocess_image(np.array(img))
        hm_row = extract_features_and_generate_hm_row(binary_image)
        hm_rows.append(hm_row)

    # Stack all HM rows and pad to required input shape if needed
    # Ensure hm_rows is 2D and convert to tensor
    input_data = torch.tensor(hm_rows, dtype=torch.float32).view(1, -1)  # Reshape to match model input

    # Make sure input size is correct
    if input_data.shape[1] < 1610:
        padding = torch.zeros((1, 1610 - input_data.shape[1]))  # Zero-pad to match model input size
        input_data = torch.cat((input_data, padding), dim=1)
    elif input_data.shape[1] > 1610:
        input_data = input_data[:, :1610]  # Trim if it's longer

    # Model prediction
    model.eval()
    with torch.no_grad():
        prediction = model(input_data)

    # Convert prediction to numpy and print results
    traits = ["Extraversion", "Agreeableness", "Conscientiousness", "Neuroticism", "Openness"]
    trait_scores = prediction.numpy().flatten()
    result = dict(zip(traits, trait_scores))

    print("Predicted Personality Traits:")
    for trait, score in result.items():
        print(f"{trait}: {score:.2f}")

    return result

# Path to your trained model file
model_path = '/content/drive/MyDrive/Colab Notebooks/Design project/personality_traits_model.pth'
model = FFM_NN()  # Initialize your model class
model.load_state_dict(torch.load(model_path))  # Load model weights

# Predict personality traits from a PDF
pdf_path = '/content/drive/MyDrive/Colab Notebooks/Design project/Dataset/202211067.pdf'  # Path to the new PDF file
predict_personality_traits(model, pdf_path)

"""Accuracy test"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import torch

# Assuming `dataset` is a DataLoader with test data, similar to the training set
model.eval()  # Set model to evaluation mode
all_labels = []
all_predictions = []

with torch.no_grad():
    for features, labels in dataset:
        predictions = model(features).squeeze(0)  # Generate predictions
        all_labels.append(labels.numpy())         # Collect actual labels
        all_predictions.append(predictions.numpy()) # Collect predictions

# Convert lists to numpy arrays for metrics calculation
all_labels = np.vstack(all_labels)
all_predictions = np.vstack(all_predictions)

# Calculate MSE, MAE, and R-squared for each trait
mse = mean_squared_error(all_labels, all_predictions, multioutput='raw_values')
mae = mean_absolute_error(all_labels, all_predictions, multioutput='raw_values')
r2 = r2_score(all_labels, all_predictions, multioutput='raw_values')

# Print metrics for each trait
traits = ["Extraversion", "Agreeableness", "Conscientiousness", "Neuroticism", "Openness"]
for i, trait in enumerate(traits):
    print(f"{trait} - MSE: {mse[i]:.4f}, MAE: {mae[i]:.4f}, R2: {r2[i]:.4f}")

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score

# Load the dataset
file_path = '/content/drive/MyDrive/Colab Notebooks/Design project/Personality traits/Updated response.csv'
data = pd.read_csv(file_path)

# Strip whitespace from column names
data.columns = data.columns.str.strip()

# Define columns for each personality trait
trait_columns = {
    "Extraversion": [
        "I see myself as someone who is talkative.",
        "I see myself as someone who is full of energy.",
        "I see myself as someone who is outgoing, sociable."
    ],
    "Agreeableness": [
        "I see myself as someone who is helpful and unselfish with others.",
        "I see myself as someone who is considerate and kind to almost everyone.",
        "I see myself as someone who likes to cooperate with others."
    ],
    "Conscientiousness": [
        "I see myself as someone who does a thorough job.",
        "I see myself as someone who makes plans and follows through with them.",
        "I see myself as someone who does things efficiently."
    ],
    "Neuroticism": [
        "I see myself as someone who worries a lot.",
        "I see myself as someone who gets nervous easily.",
        "I see myself as someone who can be moody."
    ],
    "Openness": [
        "I see myself as someone who is original, comes up with new ideas.",
        "I see myself as someone who likes to reflect, play with ideas.",
        "I see myself as someone who is curious about many different things."
    ]
}

# Calculate the average score for each trait
trait_averages = pd.DataFrame()
for trait, cols in trait_columns.items():
    trait_averages[trait] = data[cols].mean(axis=1)

# Normalize the averages to be between 0 and 1
trait_averages_normalized = trait_averages / 5

# Define the model class
class FFM_NN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(FFM_NN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.relu = nn.Tanh()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.sigmoid(self.fc2(x))
        return x

# Initialize the model
input_size = 1610  # Update this with the actual number of input features
hidden_size = 1850
output_size = 5
model = FFM_NN(input_size, hidden_size, output_size)

# Load the trained model (assuming you saved it as 'personality_model.pth')
model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Design project/personality_traits_model.pth'))
model.eval()

# Function to predict personality traits based on handwriting features
def predict_personality(features):
    features = torch.tensor(features, dtype=torch.float32)
    with torch.no_grad():
        outputs = model(features)
    return outputs.numpy()

# Example: Predict for some features (replace with actual handwriting features)
# Assuming you have a list of features for each sample (e.g., 1610 features for each sample)
sample_features = np.random.rand(1, 1610)  # Replace with actual feature data from your dataset
predictions = predict_personality(sample_features)

# Convert predictions to categories
def convert_to_category(score, thresholds):
    if score < thresholds[1]:
        return 0  # Low
    elif score < thresholds[2]:
        return 1  # Medium
    else:
        return 2  # High

# Define thresholds for each trait: Low, Medium, High
thresholds = {
    'Extraversion': [0.0, 0.33, 0.66, 1.0],  # Low, Medium, High
    'Agreeableness': [0.0, 0.33, 0.66, 1.0],
    'Conscientiousness': [0.0, 0.33, 0.66, 1.0],
    'Neuroticism': [0.0, 0.33, 0.66, 1.0],
    'Openness': [0.0, 0.33, 0.66, 1.0]
}

# Convert predicted traits to categories (treat predictions as scalars)
predicted_traits = {
    'Extraversion': predictions[0][0],  # Assuming predictions are scalar
    'Agreeableness': predictions[0][1],
    'Conscientiousness': predictions[0][2],
    'Neuroticism': predictions[0][3],
    'Openness': predictions[0][4]
}

# Convert the predictions and actual values into categories
predicted_categories = {trait: convert_to_category(pred, thresholds[trait]) for trait, pred in predicted_traits.items()}
actual_categories = {trait: convert_to_category(value, thresholds[trait]) for trait, value in trait_averages_normalized.iloc[0].items()}

# Calculate accuracy for each trait
accuracy = {trait: int(predicted_categories[trait] == actual_categories[trait]) for trait in predicted_categories}

print("Accuracy for each trait:")
for trait, acc in accuracy.items():
    print(f"{trait}: {acc}")